{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. 필수 라이브러리 설치&실행"
      ],
      "metadata": {
        "id": "ay7C6rjaJgUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGtnUlMKE6Md"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-fuzzy\n",
        "!pip install optuna\n",
        "!pip install sdv\n",
        "!pip install xgboost\n",
        "!pip install tensorflow\n",
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.single_table import CTGANSynthesizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import skfuzzy as fuzz"
      ],
      "metadata": {
        "id": "13yytnyhFDOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Train 데이터 축소(FCM, PCA) 및 이상치 처리"
      ],
      "metadata": {
        "id": "598tKzxTJ5Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/data/train.csv')\n",
        "\n",
        "features = df.select_dtypes(include=['number']).columns.difference(['ID'])\n",
        "\n",
        "m_data = df[df['Fraud_Type'] == 'm']\n",
        "other_data = df[df['Fraud_Type'] != 'm']\n",
        "\n",
        "sampled_m_data = m_data.sample(n=len(other_data), random_state=42)\n",
        "\n",
        "balanced_data = pd.concat([sampled_m_data, other_data])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(balanced_data[features])\n",
        "\n",
        "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n",
        "    scaled_data.T, c=3, m=2.5, error=0.005, maxiter=1000, init=None)\n",
        "\n",
        "cluster_labels = np.argmax(u, axis=0)\n",
        "\n",
        "balanced_data['cluster_labels'] = cluster_labels\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "balanced_data['pca_one'] = pca_result[:, 0]\n",
        "balanced_data['pca_two'] = pca_result[:, 1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(balanced_data['pca_one'], balanced_data['pca_two'], c=balanced_data['Fraud_Type'].apply(lambda x: 'red' if x == 'm' else 'blue'), alpha=0.5)\n",
        "plt.title('PCA of Dataset with Fuzzy C-Means')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n",
        "\n",
        "balanced_data = balanced_data.drop(columns=['cluster_labels'])\n",
        "\n",
        "print(balanced_data['Fraud_Type'].value_counts())"
      ],
      "metadata": {
        "id": "AarzTpI-FDMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = balanced_data.copy()\n",
        "\n",
        "print(train[\"Fraud_Type\"].value_counts())\n",
        "\n",
        "N_CLS_PER_GEN = 1000\n",
        "\n",
        "def handle_outliers(series, n_std=3):\n",
        "    mean = series.mean()\n",
        "    std = series.std()\n",
        "    z_scores = np.abs(stats.zscore(series))\n",
        "    return series.mask(z_scores > n_std, mean)\n",
        "\n",
        "train['Time_difference'] = pd.to_timedelta(train['pca_one'] * 1000, unit='s')\n",
        "\n",
        "train['Time_difference_seconds'] = train['Time_difference'].dt.total_seconds()\n",
        "train['Time_difference_seconds'] = handle_outliers(train['Time_difference_seconds'])\n",
        "\n",
        "fraud_types = train['Fraud_Type'].unique()\n"
      ],
      "metadata": {
        "id": "3q535APIFDE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. 데이터 생성(CTGAN)"
      ],
      "metadata": {
        "id": "gg6u8JFqKEEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_synthetic_data = pd.DataFrame()\n",
        "\n",
        "N_SAMPLE = 100\n",
        "\n",
        "all_synthetic_data = pd.DataFrame()\n",
        "\n",
        "for fraud_type in tqdm(fraud_types):\n",
        "\n",
        "    subset = train[train[\"Fraud_Type\"] == fraud_type]\n",
        "\n",
        "    subset = subset.sample(n=N_SAMPLE, random_state=42)\n",
        "\n",
        "    subset = subset.drop('Time_difference', axis=1)\n",
        "\n",
        "    metadata = SingleTableMetadata()\n",
        "\n",
        "    metadata.detect_from_dataframe(subset)\n",
        "    metadata.set_primary_key(None)\n",
        "\n",
        "    column_sdtypes = {\n",
        "        'Account_initial_balance': 'numerical',\n",
        "        'Account_balance': 'numerical',\n",
        "        'Customer_Gender': 'categorical',\n",
        "        'Customer_identification_number': 'categorical',\n",
        "        'Customer_personal_identifier': 'categorical',\n",
        "        'Account_account_number': 'categorical',\n",
        "        'IP_Address': 'ipv4_address',\n",
        "        'Location': 'categorical',\n",
        "        'Recipient_Account_Number': 'categorical',\n",
        "        'Fraud_Type': 'categorical',\n",
        "        'Time_difference_seconds': 'numerical',\n",
        "        'Customer_Birthyear': 'numerical'\n",
        "    }\n",
        "\n",
        "    for column, sdtype in column_sdtypes.items():\n",
        "        metadata.update_column(\n",
        "            column_name=column,\n",
        "            sdtype=sdtype\n",
        "        )\n",
        "\n",
        "    synthesizer = CTGANSynthesizer(\n",
        "        metadata,\n",
        "        epochs=100\n",
        "    )\n",
        "\n",
        "    synthesizer.fit(subset)\n",
        "\n",
        "    synthetic_subset = synthesizer.sample(num_rows=N_CLS_PER_GEN)\n",
        "\n",
        "    synthetic_subset['Time_difference_seconds'] = handle_outliers(synthetic_subset['Time_difference_seconds'])\n",
        "\n",
        "    synthetic_subset['Time_difference'] = pd.to_timedelta(synthetic_subset['Time_difference_seconds'], unit='s')\n",
        "\n",
        "    synthetic_subset = synthetic_subset.drop('Time_difference_seconds', axis=1)\n",
        "\n",
        "    all_synthetic_data = pd.concat([all_synthetic_data, synthetic_subset], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "kdJ1KTdDFDCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Train 데이터 전처리"
      ],
      "metadata": {
        "id": "qrtjWwggKN3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = balanced_data.copy()\n",
        "\n",
        "train_x = train.drop(columns=['Fraud_Type'])\n",
        "train_y = train['Fraud_Type']\n",
        "\n",
        "datetime_columns = ['Transaction_resumed_date', 'Last_atm_transaction_datetime', 'Last_bank_branch_transaction_datetime']\n",
        "for col in datetime_columns:\n",
        "    if col in train_x.columns:\n",
        "        train_x[col] = pd.to_datetime(train_x[col])\n",
        "\n",
        "train_x['Last_atm_transaction_timestamp'] = train_x['Last_atm_transaction_datetime'].astype(int) / 10**9\n",
        "train_x['Last_bank_branch_transaction_timestamp'] = train_x['Last_bank_branch_transaction_datetime'].astype(int) / 10**9\n",
        "train_x['Transaction_resumed_timestamp'] = train_x['Transaction_resumed_date'].astype(int) / 10**9\n",
        "\n",
        "train_x = train_x.drop(columns=datetime_columns)\n",
        "\n",
        "train_x['Time_difference'] = train_x['Transaction_resumed_timestamp'] - train_x['Last_atm_transaction_timestamp']\n",
        "\n",
        "le_subclass = LabelEncoder()\n",
        "train_y_encoded = le_subclass.fit_transform(train_y)\n",
        "\n",
        "for i, label in enumerate(le_subclass.classes_):\n",
        "    print(f\"원래 레이블: {label}, 변환된 숫자: {i}\")\n",
        "\n",
        "categorical_columns = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "train_x_encoded = train_x.copy()\n",
        "train_x_encoded[categorical_columns] = ordinal_encoder.fit_transform(train_x[categorical_columns])\n",
        "\n",
        "feature_order = train_x_encoded.columns.tolist()\n",
        "if 'ID' in feature_order:\n",
        "    feature_order.remove('ID')"
      ],
      "metadata": {
        "id": "AnYmLhn_FC92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. XGBoost 모델 1"
      ],
      "metadata": {
        "id": "5U96K8UbKZCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    xgb_params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 5),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
        "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0),\n",
        "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 5.0),\n",
        "        'max_delta_step': trial.suggest_float('max_delta_step', 0, 10),\n",
        "        'device': 'cpu',\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**xgb_params)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    for train_index, val_index in kf.split(train_x_encoded[feature_order], train_y_encoded):\n",
        "        X_train_fold, X_val_fold = train_x_encoded[feature_order].iloc[train_index], train_x_encoded[feature_order].iloc[val_index]\n",
        "        y_train_fold, y_val_fold = train_y_encoded[train_index], train_y_encoded[val_index]\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold, verbose=False)\n",
        "\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "        f1 = f1_score(y_val_fold, y_pred, average='macro')\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    mean_score = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "    return mean_score\n",
        "\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "\n",
        "study2 = optuna.create_study(direction='maximize', sampler = sampler)\n",
        "study2.optimize(objective, n_trials=100)\n",
        "\n",
        "best_params = study2.best_params\n",
        "print(\"Best hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "R5PJRo0kFC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best hyperparameters: {'learning_rate': 0.03500842897354163, 'n_estimators': 224, 'max_depth': 4, 'min_child_weight': 2.1605378743201147, 'gamma': 0.22804306127508525, 'subsample': 0.955950194857886, 'colsample_bytree': 0.8227035006806621, 'reg_lambda': 0.3490933288294228, 'reg_alpha': 0.21227891390474513, 'scale_pos_weight': 4.547887415904099, 'max_delta_step': 1.588355137064108}"
      ],
      "metadata": {
        "id": "d4Jzne35JYlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = xgb.XGBClassifier(**best_params, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model1.fit(train_x_encoded[feature_order], train_y_encoded)"
      ],
      "metadata": {
        "id": "hC1D0-EXJZz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 분류 성능 확인 -> i 데이터는 예측 잘 안 됨"
      ],
      "metadata": {
        "id": "2I3NIzPaFbon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/data/synthetic_data.csv'\n",
        "\n",
        "all_synthetic_data = pd.read_csv(csv_file_path)\n",
        "\n",
        "datetime_columns = ['Transaction_resumed_date', 'Last_atm_transaction_datetime', 'Last_bank_branch_transaction_datetime']\n",
        "for col in datetime_columns:\n",
        "    if col in all_synthetic_data.columns:\n",
        "        all_synthetic_data[col] = pd.to_datetime(all_synthetic_data[col])\n",
        "\n",
        "all_synthetic_data['Last_atm_transaction_timestamp'] = all_synthetic_data['Last_atm_transaction_datetime'].astype(int) / 10**9\n",
        "all_synthetic_data['Last_bank_branch_transaction_timestamp'] = all_synthetic_data['Last_bank_branch_transaction_datetime'].astype(int) / 10**9\n",
        "all_synthetic_data['Transaction_resumed_timestamp'] = all_synthetic_data['Transaction_resumed_date'].astype(int) / 10**9\n",
        "\n",
        "all_synthetic_data = all_synthetic_data.drop(columns=datetime_columns)\n",
        "\n",
        "all_synthetic_data['Time_difference'] = all_synthetic_data['Transaction_resumed_timestamp'] - all_synthetic_data['Last_atm_transaction_timestamp']\n",
        "\n",
        "all_synthetic_data_encoded = all_synthetic_data.copy()\n",
        "all_synthetic_data_encoded[categorical_columns] = ordinal_encoder.transform(all_synthetic_data[categorical_columns])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(all_synthetic_data_encoded[features])\n",
        "\n",
        "all_synthetic_data_encoded['pca_one'] = pca_result[:, 0]\n",
        "all_synthetic_data_encoded['pca_two'] = pca_result[:, 1]"
      ],
      "metadata": {
        "id": "rS9r5SA-qrpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_order = train_x.columns.tolist()\n",
        "if 'pca_one' not in feature_order:\n",
        "    feature_order.append('pca_one')\n",
        "if 'pca_two' not in feature_order:\n",
        "    feature_order.append('pca_two')\n",
        "\n",
        "all_synthetic_data_encoded = all_synthetic_data_encoded[feature_order]\n",
        "\n",
        "for col in feature_order:\n",
        "    all_synthetic_data_encoded[col] = all_synthetic_data_encoded[col].astype(train_x_encoded[col].dtype)\n",
        "\n",
        "predictions = model.predict(all_synthetic_data_encoded)\n",
        "\n",
        "predictions_label = le_subclass.inverse_transform(predictions)"
      ],
      "metadata": {
        "id": "N5qyIZ_kqrhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba_predictions = model.predict_proba(all_synthetic_data_encoded)\n",
        "\n",
        "max_proba = np.max(proba_predictions, axis=1)\n",
        "predicted_labels = np.argmax(proba_predictions, axis=1)\n",
        "\n",
        "predictions_label = le_subclass.inverse_transform(predicted_labels)\n",
        "\n",
        "comparison_df = all_synthetic_data.copy()\n",
        "comparison_df['Predicted_Fraud_Type'] = predictions_label\n",
        "comparison_df['Max_Proba'] = max_proba\n",
        "\n",
        "filtered_data = comparison_df[\n",
        "    (comparison_df['Fraud_Type'] == comparison_df['Predicted_Fraud_Type']) &\n",
        "    (comparison_df['Fraud_Type'] != 'm') &\n",
        "    (comparison_df['Max_Proba'] >= 0.8)\n",
        "\n",
        "]\n",
        "\n",
        "filtered_data = filtered_data.drop(columns=['Predicted_Fraud_Type', 'Max_Proba'])\n",
        "\n",
        "print(filtered_data['Fraud_Type'].value_counts())"
      ],
      "metadata": {
        "id": "K46-mu03tBGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. XGBoost 모델 2"
      ],
      "metadata": {
        "id": "O1BTW5m4LOVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_type_i_data = comparison_df[comparison_df['Fraud_Type'] == 'i']\n",
        "\n",
        "filtered_data = filtered_data.drop(columns=['Predicted_Fraud_Type', 'Max_Proba'])\n",
        "fraud_type_i_data = fraud_type_i_data.drop(columns=['Predicted_Fraud_Type', 'Max_Proba'])\n",
        "\n",
        "combined_data = pd.concat([filtered_data, fraud_type_i_data])\n",
        "\n",
        "combined_data_encoded = combined_data.copy()\n",
        "combined_data_encoded[categorical_columns] = ordinal_encoder.transform(combined_data[categorical_columns])"
      ],
      "metadata": {
        "id": "ggn9ZW71uVIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'learning_rate': 0.03500842897354163,\n",
        "    'n_estimators': 224,\n",
        "    'max_depth': 4,\n",
        "    'min_child_weight': 2.1605378743201147,\n",
        "    'gamma': 0.22804306127508525,\n",
        "    'subsample': 0.955950194857886,\n",
        "    'colsample_bytree': 0.8227035006806621,\n",
        "    'reg_lambda': 0.3490933288294228,\n",
        "    'reg_alpha': 0.21227891390474513,\n",
        "    'scale_pos_weight': 4.547887415904099,\n",
        "    'max_delta_step': 1.588355137064108,\n",
        "    'device': 'cuda',\n",
        "    'use_label_encoder': False,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "model2 = xgb.XGBClassifier(**params)\n",
        "\n",
        "model2.fit(train_x_encoded, train_y_encoded)"
      ],
      "metadata": {
        "id": "kwQhGsCeuUv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. 모델 앙상블"
      ],
      "metadata": {
        "id": "ZCrBC7R0L1UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "models = [\n",
        "    ('xgb1', model1),\n",
        "    ('xgb2', model2)\n",
        "]\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=models, voting='soft')\n",
        "\n"
      ],
      "metadata": {
        "id": "tdfT4G53L5Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Test 데이터 전처리 및 예측"
      ],
      "metadata": {
        "id": "UUwHdoMpKwGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = pd.read_csv('/data/test.csv')\n",
        "\n",
        "datetime_columns = ['Transaction_resumed_date', 'Last_atm_transaction_datetime', 'Last_bank_branch_transaction_datetime']\n",
        "for col in datetime_columns:\n",
        "    if col in test_x.columns:\n",
        "        test_x[col] = pd.to_datetime(test_x[col])\n",
        "\n",
        "test_x['Last_atm_transaction_timestamp'] = test_x['Last_atm_transaction_datetime'].astype(int) / 10**9\n",
        "test_x['Last_bank_branch_transaction_timestamp'] = test_x['Last_bank_branch_transaction_datetime'].astype(int) / 10**9\n",
        "test_x['Transaction_resumed_timestamp'] = test_x['Transaction_resumed_date'].astype(int) / 10**9\n",
        "\n",
        "test_x = test_x.drop(columns=datetime_columns)\n",
        "\n",
        "test_x['Time_difference'] = test_x['Transaction_resumed_timestamp'] - test_x['Last_atm_transaction_timestamp']\n",
        "\n",
        "test_x_encoded = test_x.copy()\n",
        "test_x_encoded[categorical_columns] = ordinal_encoder.transform(test_x[categorical_columns])\n",
        "\n",
        "pca_test_result = pca.transform(test_x_encoded[features])\n",
        "test_x_encoded['pca_one'] = pca_test_result[:, 0]\n",
        "test_x_encoded['pca_two'] = pca_test_result[:, 1]\n",
        "\n",
        "test_x_encoded = test_x_encoded[feature_order]\n",
        "\n",
        "for col in feature_order:\n",
        "    test_x_encoded[col] = test_x_encoded[col].astype(train_x_encoded[col].dtype)\n",
        "\n",
        "ensemble_model.fit(train_x_encoded[feature_order], train_y_encoded)\n",
        "\n",
        "with open('/data/앙상블1.pkl', 'wb') as f:\n",
        "    pickle.dump(ensemble_model, f)\n",
        "\n",
        "predictions = ensemble_model.predict(test_x_encoded)\n",
        "\n",
        "predictions_label = le_subclass.inverse_transform(predictions)"
      ],
      "metadata": {
        "id": "AUw3UsF2L4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_submission = pd.read_csv(\"/data/sample_submission.csv\")\n",
        "clf_submission[\"Fraud_Type\"] = predictions_label\n",
        "clf_submission.head()\n",
        "\n",
        "all_synthetic_data.head()\n",
        "\n",
        "os.makedirs('./submission', exist_ok=True)\n",
        "os.chdir(\"./submission/\")\n",
        "\n",
        "clf_submission.to_csv('./clf_submission.csv', encoding='UTF-8-sig', index=False)\n",
        "all_synthetic_data.to_csv('./syn_submission.csv', encoding='UTF-8-sig', index=False)\n",
        "\n",
        "syn_submission_path = './syn_submission.csv'\n",
        "syn_submission = pd.read_csv(syn_submission_path)\n",
        "columns_to_remove = ['ID', 'pca_one', 'pca_two']\n",
        "syn_submission_cleaned = syn_submission.drop(columns=[col for col in columns_to_remove if col in syn_submission.columns])\n",
        "syn_submission_cleaned.to_csv(syn_submission_path, encoding='UTF-8-sig', index=False)\n",
        "\n",
        "with zipfile.ZipFile(\"/data/submission_ensemble.zip\", 'w') as submission:\n",
        "    submission.write('clf_submission.csv')\n",
        "    submission.write('syn_submission.csv')\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "sgfCpENyePe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}